{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pramod/.local/lib/python3.5/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mat_file(matfile):\n",
    "    import scipy.io as sio\n",
    "    \n",
    "    mat_contents = sio.loadmat(matfile)\n",
    "    data = mat_contents[\"data\"]\n",
    "    return data[0,0][7]\n",
    "\n",
    "def load_matfile(img):\n",
    "    if len(img.shape)==2:\n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.concatenate([img,img,img],axis=2)\n",
    "    \n",
    "    img = imresize(img,(256,256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img /255\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3,read_mat = False):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Read image and process\n",
    "    if read_mat:\n",
    "        image = load_matfile(image_path)\n",
    "    else:\n",
    "        image = load_image(image_path)\n",
    "    \"\"\"    \n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "\n",
    "    # Lists to store completed sequences, their alphas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "        awe = gate * awe\n",
    "\n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences, alphas\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
    "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "    #print(len(complete_seqs_scores))\n",
    "    if len(complete_seqs_scores) == 0:\n",
    "        seq = [9489]\n",
    "        alphas = [0]\n",
    "    else:\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "        alphas = complete_seqs_alpha[i]\n",
    "\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder and decoder!\n"
     ]
    }
   ],
   "source": [
    "#warnings.filterwarnings('ignore')\n",
    "# Load model\n",
    "checkpoint = torch.load(\"models/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\")\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "print(\"Loaded encoder and decoder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word map (word2ix)\n",
    "with open(\"models/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\", 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # ix2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test = \"/home/pramod/Downloads/datasets/image_caption/pascal_image_data/test/\"\n",
    "image_eg = \"image_1.jpg\"\n",
    "seq, alphas = caption_image_beam_search(encoder, decoder, images_test+image_eg, word_map, beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'a', 'man', 'with', 'a', 'beard', 'is', 'holding', 'a', 'dog', '<end>']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "words = [rev_word_map[ind] for ind in seq]\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_1.jpg\n",
      "[9488, 1, 2, 3, 1, 4070, 35, 55, 1, 974, 9489]\n",
      "['<start>', 'a', 'man', 'with', 'a', 'beard', 'is', 'holding', 'a', 'dog', '<end>']\n",
      "image_1449.jpg\n",
      "[9488, 1, 570, 86, 17, 1, 68, 32, 1, 716, 9489]\n",
      "['<start>', 'a', 'close', 'up', 'of', 'a', 'cow', 'in', 'a', 'field', '<end>']\n",
      "image_19.jpg\n",
      "[9488, 1, 974, 370, 6, 14, 587, 61, 23, 1, 974, 9489]\n",
      "['<start>', 'a', 'dog', 'laying', 'on', 'the', 'ground', 'next', 'to', 'a', 'dog', '<end>']\n",
      "image_2349.jpg\n",
      "[9488, 1, 893, 123, 3, 1, 896, 28, 1, 2653, 9489]\n",
      "['<start>', 'a', 'living', 'room', 'with', 'a', 'tv', 'and', 'a', 'fireplace', '<end>']\n",
      "image_28.jpg\n",
      "[9488, 1, 207, 799, 71, 61, 23, 1, 1975, 427, 9489]\n",
      "['<start>', 'a', 'brown', 'horse', 'standing', 'next', 'to', 'a', 'wire', 'fence', '<end>']\n",
      "image_3249.jpg\n",
      "[9488, 1, 124, 98, 6, 1, 1603, 32, 1, 123, 9489]\n",
      "['<start>', 'a', 'cat', 'sitting', 'on', 'a', 'bed', 'in', 'a', 'room', '<end>']\n",
      "image_37.jpg\n",
      "[9488, 1, 645, 409, 6, 14, 37, 17, 1, 289, 9489]\n",
      "['<start>', 'a', 'car', 'parked', 'on', 'the', 'side', 'of', 'a', 'street', '<end>']\n",
      "image_4149.jpg\n",
      "[9488, 1, 974, 50, 35, 370, 371, 6, 1, 1778, 9489]\n",
      "['<start>', 'a', 'dog', 'that', 'is', 'laying', 'down', 'on', 'a', 'blanket', '<end>']\n",
      "image_46.jpg\n",
      "[9488, 1, 96, 17, 97, 11, 794, 32, 1, 716, 9489]\n",
      "['<start>', 'a', 'group', 'of', 'people', 'riding', 'horses', 'in', 'a', 'field', '<end>']\n",
      "image_592.jpg\n",
      "[9488, 1, 360, 17, 97, 50, 179, 98, 79, 1, 102, 9489]\n",
      "['<start>', 'a', 'couple', 'of', 'people', 'that', 'are', 'sitting', 'at', 'a', 'table', '<end>']\n"
     ]
    }
   ],
   "source": [
    "show_tell_attend_df  = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "images = \"/home/pramod/Downloads/datasets/image_caption/pascal_image_data/test/\"\n",
    "for img in os.listdir(images):\n",
    "        if os.path.splitext(img)[1] == '.png' or os.path.splitext(img)[1] =='.jpeg' or os.path.splitext(img)[1] =='.jpg':\n",
    "    \n",
    "            seq, _ = caption_image_beam_search(encoder, decoder, images+img, word_map, beam_size=5)\n",
    "            words = [rev_word_map[ind] for ind in seq]\n",
    "            show_tell_attend_df=show_tell_attend_df.append(\n",
    "            pd.DataFrame({\"img\":img,\"show_attend_tell\":str(words)},index=[0]),ignore_index=True)\n",
    "            if count %500 ==0:\n",
    "                print(img)\n",
    "                print(seq)\n",
    "                print(words)\n",
    "            count +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matfile = '/home/pramod/Downloads/datasets/image_caption/labelme/data_for_image_caption.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = False\n",
    "if mat_data:\n",
    "    #read matfile\n",
    "    img_list = read_mat_file(matfile)\n",
    "    print(len(img_list[0]))\n",
    "    #perform caption generation\n",
    "    show_tell_attend_df  = pd.DataFrame()\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(img_list[0])):\n",
    "        seq, _ = caption_image_beam_search(encoder, decoder, img_list[0][i], word_map, beam_size=5,read_mat=False)\n",
    "        words = [rev_word_map[ind] for ind in seq]\n",
    "        show_tell_attend_df=show_tell_attend_df.append(\n",
    "        pd.DataFrame({\"img\":i,\"show_attend_tell\":str(words)},index=[0]),ignore_index=True)\n",
    "        if count %500 ==0:\n",
    "            print(i)\n",
    "            print(seq)\n",
    "            print(words)\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4952, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tell_attend_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tell_attend_df.to_csv(\"pascal_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4952, 3)\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"../show_tell/pascal_test.csv\")\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(results,show_tell_attend_df,on=\"img\").to_csv(\"pascal_test_show_attend_tell.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>show_attend_tell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>['&lt;start&gt;', 'a', 'man', 'with', 'a', 'beard', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_10.jpg</td>\n",
       "      <td>['&lt;start&gt;', 'a', 'yellow', 'taxi', 'cab', 'dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_100.jpg</td>\n",
       "      <td>['&lt;start&gt;', 'a', 'close', 'up', 'of', 'a', 'bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_1000.jpg</td>\n",
       "      <td>['&lt;start&gt;', 'a', 'group', 'of', 'people', 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_1001.jpg</td>\n",
       "      <td>['&lt;start&gt;', 'a', 'green', 'bus', 'parked', 'on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              img                                   show_attend_tell\n",
       "0     image_1.jpg  ['<start>', 'a', 'man', 'with', 'a', 'beard', ...\n",
       "1    image_10.jpg  ['<start>', 'a', 'yellow', 'taxi', 'cab', 'dri...\n",
       "2   image_100.jpg  ['<start>', 'a', 'close', 'up', 'of', 'a', 'bi...\n",
       "3  image_1000.jpg  ['<start>', 'a', 'group', 'of', 'people', 'sta...\n",
       "4  image_1001.jpg  ['<start>', 'a', 'green', 'bus', 'parked', 'on..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tell_attend_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6223090127514708 0.6222160789161449\n"
     ]
    }
   ],
   "source": [
    "#reference - human translation\n",
    "#candidate - machine translation\n",
    "\n",
    "reference = show_tell_attend_df.iloc[1,1]\n",
    "candidate = show_tell_attend_df.iloc[0,1]\n",
    "score1 = sentence_bleu([reference], candidate)\n",
    "score2 = sentence_bleu([candidate],reference)\n",
    "print(score1, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4952, 4952)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_list = [show_tell_attend_df.iloc[i,1] for i in range(show_tell_attend_df.shape[0])]\n",
    "sentence_scores = np.zeros((len(sentences_list),show_tell_attend_df.shape[0]))\n",
    "sentence_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0 0\n",
      "0.584908045268514 4950 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-ba5d0c7a48b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m#print(idx,sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#print(show_tell_attend_df.iloc[i,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msentence_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_tell_attend_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m4950\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35th/lib/python3.5/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36msentence_bleu\u001b[0;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m    100\u001b[0m     return corpus_bleu([references], [hypothesis],\n\u001b[0;32m--> 101\u001b[0;31m                        weights, smoothing_function, auto_reweigh)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35th/lib/python3.5/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# denominator for the corpus-level modified precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mp_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35th/lib/python3.5/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     clipped_counts = {ngram: min(count, max_counts[ngram])\n\u001b[0;32m--> 315\u001b[0;31m                       for ngram, count in counts.items()}\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35th/lib/python3.5/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     clipped_counts = {ngram: min(count, max_counts[ngram])\n\u001b[0;32m--> 315\u001b[0;31m                       for ngram, count in counts.items()}\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx,sentence in enumerate(sentences_list):\n",
    "    for i in range(show_tell_attend_df.shape[0]):\n",
    "        #print(idx,sentence)\n",
    "        #print(show_tell_attend_df.iloc[i,1])\n",
    "        sentence_scores[idx][i] = sentence_bleu([sentence],show_tell_attend_df.iloc[i,1])\n",
    "        if i%4950 == 0 and idx%200==0:\n",
    "            print(sentence_scores[idx][i],i,idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"sentence_scores_dataset.csv\",sentence_scores,fmt='%.4f',delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
